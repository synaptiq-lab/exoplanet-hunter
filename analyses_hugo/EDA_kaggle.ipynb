{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA Polars — KOI & TOI (Notebook Kaggle autonome)\n",
        "\n",
        "Ce notebook:\n",
        "- installe les dépendances nécessaires (Polars, Matplotlib, Seaborn)\n",
        "- charge les CSV KOI/TOI (à déposer dans l'environnement Kaggle ou via un Dataset Kaggle)\n",
        "- réalise: structure/manquants/distributions, détection de problèmes (outliers IQR, duplicats, déséquilibre), corrélations et figures inline\n",
        "\n",
        "Exécutez les cellules dans l'ordre; adaptez les chemins des fichiers si vous utilisez un Dataset Kaggle (`/kaggle/input/...`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation des dépendances (Kaggle)\n",
        "# - polars: traitement de données rapide (colonnaire)\n",
        "# - matplotlib/seaborn: visualisations\n",
        "!pip -q install polars matplotlib seaborn --upgrade\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports et configuration globale\n",
        "# - pl/pd: manipulation de données (Polars prioritaire)\n",
        "# - matplotlib/seaborn: graphiques\n",
        "# - display/Markdown: affichage propre dans le notebook\n",
        "import os\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Style par défaut pour les figures\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# Répertoires de sortie (les figures seront sauvegardées ici)\n",
        "OUT_DIR = \"./outputs\"\n",
        "FIGS_DIR = os.path.join(OUT_DIR, \"figures\")\n",
        "TABLES_DIR = os.path.join(OUT_DIR, \"tables\")\n",
        "os.makedirs(FIGS_DIR, exist_ok=True)\n",
        "os.makedirs(TABLES_DIR, exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonctions utilitaires (autonomes)\n",
        "# - sniff_delimiter: détection robuste du séparateur CSV\n",
        "# - read_csv_pl: lecture KOI/TOI avec Polars (fallback Pandas)\n",
        "# - get_*_columns: identification colonnes numériques/catégorielles\n",
        "# - schema_and_missing: synthèse types et valeurs manquantes\n",
        "# - numeric_distributions: statistiques + quantiles\n",
        "# - outliers_iqr: détection outliers par règle IQR\n",
        "# - duplicates_summary: comptage + échantillon de doublons\n",
        "# - class_imbalance: top catégorie dominante et part\n",
        "# - correlations/high_correlation_pairs: matrice de corrélation et paires |r|≥seuil\n",
        "# - save_*: génération des figures (histogrammes, barplots, heatmap)\n",
        "import csv\n",
        "\n",
        "def sniff_delimiter(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        head = f.read(64 * 1024)\n",
        "    try:\n",
        "        dialect = csv.Sniffer().sniff(head, delimiters=\",;\\t|\")\n",
        "        return dialect.delimiter\n",
        "    except Exception:\n",
        "        return \",\"\n",
        "\n",
        "def read_csv_pl(path: str) -> pl.DataFrame:\n",
        "    sep = sniff_delimiter(path)\n",
        "    for enc in (\"utf8\", \"utf8-lossy\"):\n",
        "        try:\n",
        "            return pl.read_csv(path, separator=sep, encoding=enc, infer_schema_length=5000, ignore_errors=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "    # fallback pandas -> polars (tolère lignes corrompues)\n",
        "    df_pd = pd.read_csv(path, sep=sep, on_bad_lines=\"skip\")\n",
        "    return pl.from_pandas(df_pd)\n",
        "\n",
        "def get_numeric_columns(df: pl.DataFrame):\n",
        "    return [c for c, dt in zip(df.columns, df.dtypes) if pl.datatypes.is_numeric(dt)]\n",
        "\n",
        "def get_categorical_columns(df: pl.DataFrame, max_unique: int = 50):\n",
        "    cats = []\n",
        "    for c, dt in zip(df.columns, df.dtypes):\n",
        "        if pl.datatypes.is_utf8(dt) or pl.datatypes.is_boolean(dt):\n",
        "            # Nombre de modalités distinctes (limité pour éviter les cardinalités trop élevées)\n",
        "            n_unique = int(df.select(pl.col(c).n_unique()).item())\n",
        "            if n_unique <= max_unique:\n",
        "                cats.append(c)\n",
        "    return cats\n",
        "\n",
        "def schema_and_missing(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    rows = []\n",
        "    n = df.height\n",
        "    for c, dt in zip(df.columns, df.dtypes):\n",
        "        nulls = int(df.select(pl.col(c).is_null().sum()).item())\n",
        "        rows.append({\"column\": c, \"dtype\": str(dt), \"null_count\": nulls, \"null_pct\": (nulls / n) if n else 0.0})\n",
        "    return pl.DataFrame(rows)\n",
        "\n",
        "def numeric_distributions(df: pl.DataFrame, numeric_cols):\n",
        "    qs = [0.0, 0.25, 0.5, 0.75, 0.9, 0.95, 1.0]\n",
        "    out_rows = []\n",
        "    for c in numeric_cols:\n",
        "        desc = df.select([\n",
        "            pl.col(c).count().alias(\"count\"),\n",
        "            pl.col(c).mean().alias(\"mean\"),\n",
        "            pl.col(c).std().alias(\"std\"),\n",
        "            pl.col(c).min().alias(\"min\"),\n",
        "            pl.col(c).max().alias(\"max\"),\n",
        "        ]).to_dicts()[0]\n",
        "        quants = df.select(pl.col(c).quantile(qs)).to_series().to_list()\n",
        "        row = {\"column\": c, **desc}\n",
        "        for q, val in zip(qs, quants):\n",
        "            row[f\"q{int(q*100):02d}\"] = val\n",
        "        out_rows.append(row)\n",
        "    return pl.DataFrame(out_rows)\n",
        "\n",
        "def outliers_iqr(df: pl.DataFrame, numeric_cols):\n",
        "    rows = []\n",
        "    n = df.height\n",
        "    for c in numeric_cols:\n",
        "        q1 = df.select(pl.col(c).quantile(0.25)).item()\n",
        "        q3 = df.select(pl.col(c).quantile(0.75)).item()\n",
        "        if q1 is None or q3 is None:\n",
        "            rows.append({\"column\": c, \"lower\": None, \"upper\": None, \"outliers\": 0, \"outlier_pct\": 0.0})\n",
        "            continue\n",
        "        iqr = q3 - q1\n",
        "        lower = q1 - 1.5 * iqr\n",
        "        upper = q3 + 1.5 * iqr\n",
        "        cnt = int(df.select(((pl.col(c) < lower) | (pl.col(c) > upper)).sum()).item())\n",
        "        rows.append({\"column\": c, \"lower\": lower, \"upper\": upper, \"outliers\": cnt, \"outlier_pct\": (cnt / n) if n else 0.0})\n",
        "    return pl.DataFrame(rows)\n",
        "\n",
        "def duplicates_summary(df: pl.DataFrame):\n",
        "    n = df.height\n",
        "    n_unique_rows = int(df.unique().height)\n",
        "    dup_count = n - n_unique_rows\n",
        "    sample = None\n",
        "    if dup_count > 0:\n",
        "        sample = (\n",
        "            df.with_row_count()\n",
        "              .groupby(df.columns)\n",
        "              .len()\n",
        "              .filter(pl.col(\"len\") > 1)\n",
        "              .limit(30)\n",
        "        )\n",
        "    return {\"total\": n, \"duplicates\": dup_count, \"sample\": sample}\n",
        "\n",
        "def class_imbalance(df: pl.DataFrame, categorical_cols):\n",
        "    rows = []\n",
        "    for c in categorical_cols:\n",
        "        vc = df.group_by(c).len().sort(\"len\", descending=True)\n",
        "        total = int(vc.select(pl.col(\"len\").sum()).item())\n",
        "        if total == 0:\n",
        "            continue\n",
        "        top = vc.row(0)\n",
        "        rows.append({\"column\": c, \"top_value\": str(top[0]), \"top_count\": int(top[1]), \"top_pct\": int(top[1]) / total})\n",
        "    return pl.DataFrame(rows)\n",
        "\n",
        "def correlations(df: pl.DataFrame, numeric_cols):\n",
        "    # Corrélation Pearson (conversion pandas pour compatibilité)\n",
        "    corr = df.select(numeric_cols).to_pandas().corr(method=\"pearson\")\n",
        "    return pl.from_pandas(corr)\n",
        "\n",
        "def high_correlation_pairs(corr_df: pl.DataFrame, threshold: float = 0.9):\n",
        "    cols = list(corr_df.columns)\n",
        "    rows = []\n",
        "    pdf = corr_df.to_pandas()\n",
        "    for i in range(len(cols)):\n",
        "        for j in range(i + 1, len(cols)):\n",
        "            r = pdf.iloc[i, j]\n",
        "            if r is not None and abs(r) >= threshold:\n",
        "                rows.append({\"var1\": cols[i], \"var2\": cols[j], \"corr\": float(r)})\n",
        "    return pl.DataFrame(rows)\n",
        "\n",
        "def save_histograms(df: pl.DataFrame, numeric_cols, out_dir: str, max_plots: int = 12, name_prefix: str = \"\"):\n",
        "    # Histogrammes sur les variables numériques (tri par variance décroissante)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    paths = []\n",
        "    variances = []\n",
        "    for c in numeric_cols:\n",
        "        try:\n",
        "            var_val = df.select(pl.col(c).var()).item()\n",
        "            if var_val is None:\n",
        "                continue\n",
        "            variances.append((c, float(var_val)))\n",
        "        except Exception:\n",
        "            continue\n",
        "    variances.sort(key=lambda kv: kv[1], reverse=True)\n",
        "    for c, _ in variances[:max_plots]:\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "        try:\n",
        "            data = df[c].drop_nulls().to_numpy()\n",
        "            ax.hist(data, bins=50, color=\"#4a90e2\", alpha=0.85)\n",
        "            ax.set_title(f\"Histogramme: {c}\")\n",
        "            ax.set_xlabel(c)\n",
        "            ax.set_ylabel(\"Fréquence\")\n",
        "            fig.tight_layout()\n",
        "            prefix = f\"{name_prefix}_\" if name_prefix else \"\"\n",
        "            out_path = os.path.join(out_dir, f\"{prefix}hist_{c}.png\")\n",
        "            fig.savefig(out_path, dpi=140)\n",
        "            paths.append(out_path)\n",
        "        finally:\n",
        "            plt.close(fig)\n",
        "    return paths\n",
        "\n",
        "def save_barplots(df: pl.DataFrame, categorical_cols, out_dir: str, top_k: int = 15, name_prefix: str = \"\"):\n",
        "    # Barplots des top modalités par colonne catégorielle\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    paths = []\n",
        "    for c in categorical_cols:\n",
        "        vc = df.group_by(c).len().sort(\"len\", descending=True).limit(top_k)\n",
        "        labels = [str(row[0]) for row in vc.iter_rows()]\n",
        "        values = [int(row[1]) for row in vc.iter_rows()]\n",
        "        fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "        ax.barh(range(len(values)), values, color=\"#7b9acc\")\n",
        "        ax.set_yticks(range(len(labels)))\n",
        "        ax.set_yticklabels(labels, fontsize=8)\n",
        "        ax.invert_yaxis()\n",
        "        ax.set_title(f\"Top catégories: {c}\")\n",
        "        ax.set_xlabel(\"Comptes\")\n",
        "        fig.tight_layout()\n",
        "        prefix = f\"{name_prefix}_\" if name_prefix else \"\"\n",
        "        safe_c = c.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "        out_path = os.path.join(out_dir, f\"{prefix}bar_{safe_c}.png\")\n",
        "        fig.savefig(out_path, dpi=140)\n",
        "        paths.append(out_path)\n",
        "        plt.close(fig)\n",
        "    return paths\n",
        "\n",
        "def save_corr_heatmap(corr_df: pl.DataFrame, out_path: str, name_prefix: str = \"\"):\n",
        "    # Heatmap de la matrice de corrélations\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    corr_pd = corr_df.to_pandas()\n",
        "    fig, ax = plt.subplots(figsize=(9, 7))\n",
        "    im = ax.imshow(corr_pd.values, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "    ax.set_xticks(range(len(corr_pd.columns)))\n",
        "    ax.set_yticks(range(len(corr_pd.index)))\n",
        "    ax.set_xticklabels(corr_pd.columns, rotation=90, fontsize=7)\n",
        "    ax.set_yticklabels(corr_pd.index, fontsize=7)\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    fig.tight_layout()\n",
        "    base_dir = os.path.dirname(out_path)\n",
        "    base_name = os.path.basename(out_path)\n",
        "    if name_prefix:\n",
        "        base_name = f\"{name_prefix}_\" + base_name\n",
        "    final_path = os.path.join(base_dir, base_name)\n",
        "    fig.savefig(final_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "    return final_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chemins d'entrées (à adapter sur Kaggle si besoin)\n",
        "KOI_PATH = \"./KOI.csv\"  # ou '/kaggle/input/<dataset>/KOI.csv'\n",
        "TOI_PATH = \"./TOI.csv\"  # ou '/kaggle/input/<dataset>/TOI.csv'\n",
        "\n",
        "# Chargement\n",
        "try:\n",
        "    df_koi = read_csv_pl(KOI_PATH)\n",
        "    display(Markdown(f\"**Lecture KOI**: `{KOI_PATH}` — {df_koi.height} lignes, {len(df_koi.columns)} colonnes\"))\n",
        "except Exception as e:\n",
        "    display(Markdown(f\"**Erreur de lecture KOI**: {e}\"))\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    df_toi = read_csv_pl(TOI_PATH)\n",
        "    display(Markdown(f\"**Lecture TOI**: `{TOI_PATH}` — {df_toi.height} lignes, {len(df_toi.columns)} colonnes\"))\n",
        "except Exception as e:\n",
        "    display(Markdown(f\"**Erreur de lecture TOI**: {e}\"))\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Comprendre les données — structure & distributions\n",
        "\n",
        "def overview(df: pl.DataFrame, name: str):\n",
        "    display(Markdown(f\"### {name} — Structure & manquants\"))\n",
        "    schema_df = schema_and_missing(df)\n",
        "    display(schema_df.to_pandas().head(20))\n",
        "    num_cols = get_numeric_columns(df)\n",
        "    num_summary = numeric_distributions(df, num_cols)\n",
        "    display(Markdown(f\"### {name} — Distributions numériques (résumé)\"))\n",
        "    display(num_summary.to_pandas().head(20))\n",
        "\n",
        "overview(df_koi, \"KOI\")\n",
        "overview(df_toi, \"TOI\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Problèmes — outliers, duplicats, déséquilibres\n",
        "\n",
        "def problems(df: pl.DataFrame, name: str):\n",
        "    num_cols = get_numeric_columns(df)\n",
        "    out_iqr = outliers_iqr(df, num_cols)\n",
        "    display(Markdown(f\"### {name} — Outliers (IQR)\"))\n",
        "    display(out_iqr.sort(by=[\"outlier_pct\"], reverse=True).to_pandas().head(20))\n",
        "\n",
        "    dups = duplicates_summary(df)\n",
        "    display(Markdown(f\"### {name} — Duplicats\"))\n",
        "    display({\"total\": dups[\"total\"], \"duplicates\": dups[\"duplicates\"]})\n",
        "    if dups.get(\"sample\") is not None:\n",
        "        display(Markdown(\"Échantillon duplicats (max 30):\"))\n",
        "        display(dups[\"sample\"].to_pandas())\n",
        "\n",
        "    cat_cols = get_categorical_columns(df, max_unique=20)\n",
        "    class_imb = class_imbalance(df, cat_cols)\n",
        "    display(Markdown(f\"### {name} — Déséquilibre de classes\"))\n",
        "    display(class_imb.sort(by=[\"top_pct\"], reverse=True).to_pandas().head(20))\n",
        "\n",
        "problems(df_koi, \"KOI\")\n",
        "problems(df_toi, \"TOI\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Relations — corrélations & figures\n",
        "\n",
        "def relations(df: pl.DataFrame, name: str):\n",
        "    num_cols = get_numeric_columns(df)\n",
        "    corr_df = correlations(df, num_cols)\n",
        "    display(Markdown(f\"### {name} — Corrélations (aperçu)\"))\n",
        "    display(corr_df.to_pandas().head(10))\n",
        "\n",
        "    high_pairs = high_correlation_pairs(corr_df, threshold=0.9)\n",
        "    display(Markdown(f\"### {name} — Paires très corrélées (|r|≥0.9)\"))\n",
        "    display(high_pairs.to_pandas().head(30))\n",
        "\n",
        "    hist_paths = save_histograms(df, num_cols, FIGS_DIR, max_plots=12, name_prefix=name)\n",
        "    heatmap_path = save_corr_heatmap(corr_df, os.path.join(FIGS_DIR, \"corr_heatmap.png\"), name_prefix=name)\n",
        "    bar_paths = save_barplots(df, get_categorical_columns(df, 20), FIGS_DIR, top_k=12, name_prefix=name)\n",
        "\n",
        "    display(Markdown(\"#### Figures enregistrées\"))\n",
        "    display({\n",
        "        \"heatmap\": heatmap_path,\n",
        "        \"hist\": hist_paths[:5],\n",
        "        \"bar\": bar_paths[:5],\n",
        "    })\n",
        "\n",
        "relations(df_koi, \"KOI\")\n",
        "relations(df_toi, \"TOI\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Affichages inline — helpers (hist, barplots, heatmap)\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "def show_hist_inline(df: pl.DataFrame, cols, bins: int = 50, max_plots: int = 9):\n",
        "    cols = list(cols)[:max_plots]\n",
        "    n = len(cols)\n",
        "    if n == 0:\n",
        "        display(Markdown(\"Aucune colonne numérique à tracer.\"))\n",
        "        return\n",
        "    rows, cols_per_row = math.ceil(n / 3), 3\n",
        "    fig, axes = plt.subplots(rows, cols_per_row, figsize=(15, 4 * rows))\n",
        "    try:\n",
        "        axes = axes.flatten()\n",
        "    except Exception:\n",
        "        axes = [axes]\n",
        "    for ax, col in zip(axes, cols):\n",
        "        s = df[col].drop_nulls().to_numpy()\n",
        "        ax.hist(s, bins=bins, color=\"#4a90e2\", alpha=0.85)\n",
        "        ax.set_title(col, fontsize=9)\n",
        "    for k in range(len(cols), len(axes)):\n",
        "        fig.delaxes(axes[k])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_barplots_inline(df: pl.DataFrame, cat_cols, top_k: int = 12, max_plots: int = 6):\n",
        "    cat_cols = list(cat_cols)[:max_plots]\n",
        "    for col in cat_cols:\n",
        "        vc = df.group_by(col).len().sort(\"len\", descending=True).limit(top_k)\n",
        "        labels = [str(row[0]) for row in vc.iter_rows()]\n",
        "        counts = [int(row[1]) for row in vc.iter_rows()]\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.barh(labels, counts, color=\"#7b9acc\")\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.title(col)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def show_corr_heatmap_inline(df: pl.DataFrame, num_cols):\n",
        "    if not num_cols:\n",
        "        display(Markdown(\"Pas de colonnes numériques pour la corrélation.\"))\n",
        "        return\n",
        "    corr = df.select(num_cols).to_pandas().corr(\"pearson\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "    plt.title(\"Matrice de corrélation\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Afficher inline — KOI\n",
        "num_cols_koi = get_numeric_columns(df_koi)\n",
        "cat_cols_koi = get_categorical_columns(df_koi, 20)\n",
        "\n",
        "display(Markdown(\"## KOI — Histogrammes\"))\n",
        "show_hist_inline(df_koi, num_cols_koi, bins=50, max_plots=9)\n",
        "\n",
        "display(Markdown(\"## KOI — Barplots\"))\n",
        "show_barplots_inline(df_koi, cat_cols_koi, top_k=12, max_plots=6)\n",
        "\n",
        "display(Markdown(\"## KOI — Heatmap corrélation\"))\n",
        "show_corr_heatmap_inline(df_koi, num_cols_koi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Afficher inline — TOI\n",
        "num_cols_toi = get_numeric_columns(df_toi)\n",
        "cat_cols_toi = get_categorical_columns(df_toi, 20)\n",
        "\n",
        "display(Markdown(\"## TOI — Histogrammes\"))\n",
        "show_hist_inline(df_toi, num_cols_toi, bins=50, max_plots=9)\n",
        "\n",
        "display(Markdown(\"## TOI — Barplots\"))\n",
        "show_barplots_inline(df_toi, cat_cols_toi, top_k=12, max_plots=6)\n",
        "\n",
        "display(Markdown(\"## TOI — Heatmap corrélation\"))\n",
        "show_corr_heatmap_inline(df_toi, num_cols_toi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Afficher les images sauvegardées (si vous avez utilisé les fonctions save_*)\n",
        "from IPython.display import Image\n",
        "import glob, os\n",
        "\n",
        "# Heatmaps\n",
        "for p in sorted(glob.glob(os.path.join(FIGS_DIR, \"*corr_heatmap*.png\"))):\n",
        "    display(Markdown(f\"#### {os.path.basename(p)}\"))\n",
        "    display(Image(filename=p))\n",
        "\n",
        "# Quelques histogrammes\n",
        "for p in sorted(glob.glob(os.path.join(FIGS_DIR, \"*hist_*.png\")))[:10]:\n",
        "    display(Markdown(f\"#### {os.path.basename(p)}\"))\n",
        "    display(Image(filename=p))\n",
        "\n",
        "# Quelques barplots\n",
        "for p in sorted(glob.glob(os.path.join(FIGS_DIR, \"*bar_*.png\")))[:10]:\n",
        "    display(Markdown(f\"#### {os.path.basename(p)}\"))\n",
        "    display(Image(filename=p))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
