"""
Script de test pour g√©n√©rer UN SEUL CSV avec tous les fichiers FITS
"""

from pathlib import Path
import re
import time
import gc

import pandas as pd
from tqdm import tqdm
import concurrent.futures
import lightkurve as lk


def extract_tic_from_filename(filename):
    """
    Extrait le TIC depuis le nom de fichier TESS.
    Format: tess*-s*-*TIC*-*-s_lc.fits
    """
    pattern = r"-0+(\d+)-"
    match = re.search(pattern, filename)
    if match:
        return int(match.group(1))
    return None


def extract_sector_from_filename(filename):
    """
    Extrait le num√©ro de secteur depuis le nom de fichier TESS.
    Format: tess*-s0092-*-s_lc.fits o√π 0092 est le secteur
    """
    pattern = r"-s(\d+)-"
    match = re.search(pattern, filename)
    if match:
        return int(match.group(1))
    return None


def extract_dataframe_from_fits(fits_path):
    """
    Extrait le DataFrame d'un fichier FITS avec TIC et SECTOR.
    
    Args:
        fits_path (Path): Chemin vers le fichier FITS
    
    Returns:
        tuple: (DataFrame, dict) - DataFrame ou None si erreur, et r√©sultat
    """
    try:
        # Extraire le TIC depuis le nom de fichier
        tic = extract_tic_from_filename(fits_path.name)
        
        if tic is None:
            return None, {
                'status': 'failed',
                'filename': fits_path.name,
                'error': 'Impossible d\'extraire le TIC'
            }
        
        # Extraire le secteur depuis le nom de fichier
        sector = extract_sector_from_filename(fits_path.name)
        
        if sector is None:
            return None, {
                'status': 'failed',
                'filename': fits_path.name,
                'error': 'Impossible d\'extraire le SECTOR'
            }
        
        # Lire le fichier FITS avec lightkurve
        lc = lk.read(str(fits_path))
        
        # Convertir en DataFrame pandas
        df = lc.to_pandas()
        
        # IMPORTANT: R√©initialiser l'index pour que 'time' devienne une colonne
        # Si l'index s'appelle 'time', on le transforme en colonne
        if df.index.name == 'time' or (hasattr(df.index, 'name') and df.index.name is not None):
            df = df.reset_index()
        
        # Ajouter les colonnes TIC et SECTOR √† la fin (pas au d√©but)
        # Cela pr√©serve toutes les colonnes originales
        df['TIC'] = tic
        df['SECTOR'] = sector
        
        return df, {
            'status': 'success',
            'filename': fits_path.name,
            'tic': tic,
            'sector': sector,
            'rows': len(df)
        }
        
    except Exception as e:
        return None, {
            'status': 'failed',
            'filename': fits_path.name,
            'error': str(e)
        }


def process_single_fits(fits_path, output_dir, stats_lock=None, stats=None):
    """
    Traite un seul fichier FITS et g√©n√®re un CSV avec les donn√©es
    de la courbe de lumi√®re et le TIC.
    
    Args:
        fits_path (Path): Chemin vers le fichier FITS
        output_dir (Path): Dossier de sortie pour les CSV
        stats_lock (Lock): Lock pour mise √† jour thread-safe des stats
        stats (dict): Dictionnaire de statistiques
    
    Returns:
        dict: R√©sultat du traitement
    """
    try:
        # Extraire le TIC depuis le nom de fichier
        tic = extract_tic_from_filename(fits_path.name)
        
        if tic is None:
            return {
                'status': 'failed',
                'filename': fits_path.name,
                'error': 'Impossible d\'extraire le TIC'
            }
        
        # Extraire le secteur depuis le nom de fichier
        sector = extract_sector_from_filename(fits_path.name)
        
        if sector is None:
            return {
                'status': 'failed',
                'filename': fits_path.name,
                'error': 'Impossible d\'extraire le SECTOR'
            }
        
        # Cr√©er le nom de fichier de sortie
        output_filename = fits_path.stem + '_LIGHTCURVE_data.csv'
        output_path = output_dir / output_filename
        
        # Si le fichier existe d√©j√†, le skipper
        if output_path.exists():
            if stats_lock and stats:
                with stats_lock:
                    stats['skipped'] += 1
            return {
                'status': 'skipped',
                'filename': fits_path.name,
                'output': str(output_path)
            }
        
        # Lire le fichier FITS avec lightkurve
        lc = lk.read(str(fits_path))
        
        # Convertir en DataFrame pandas
        df = lc.to_pandas()
        
        # IMPORTANT: R√©initialiser l'index pour que 'time' devienne une colonne
        # Si l'index s'appelle 'time', on le transforme en colonne
        if df.index.name == 'time' or (hasattr(df.index, 'name') and df.index.name is not None):
            df = df.reset_index()
        
        # Ajouter les colonnes TIC et SECTOR √† la fin (pas au d√©but)
        # Cela pr√©serve toutes les colonnes originales
        df['TIC'] = tic
        df['SECTOR'] = sector
        
        # Cr√©er le dossier de sortie s'il n'existe pas
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Sauvegarder en CSV
        df.to_csv(output_path, index=False)
        
        if stats_lock and stats:
            with stats_lock:
                stats['success'] += 1
        
        return {
            'status': 'success',
            'filename': fits_path.name,
            'output': str(output_path),
            'tic': tic,
            'sector': sector,
            'rows': len(df)
        }
        
    except Exception as e:
        if stats_lock and stats:
            with stats_lock:
                stats['failed'] += 1
        return {
            'status': 'failed',
            'filename': fits_path.name,
            'error': str(e)
        }


def process_all_fits_single_csv(fits_dir, output_file='all_lightcurves.csv', max_workers=3, progress_bar=True, batch_size=100, keep_batches=False):
    """
    Traite tous les fichiers FITS et g√©n√®re UN SEUL fichier CSV avec toutes les donn√©es.
    Strat√©gie optimis√©e : cr√©e des fichiers CSV interm√©diaires par batch, puis les fusionne.
    Cette approche est plus r√©siliente aux crashes et permet de reprendre en cas d'erreur.
    
    Args:
        fits_dir (Path or str): Dossier contenant les fichiers FITS
        output_file (str): Nom du fichier CSV de sortie final
        max_workers (int): Nombre de threads parall√®les (d√©faut: 3, max recommand√©: 4)
        progress_bar (bool): Afficher la barre de progression
        batch_size (int): Nombre de DataFrames √† accumuler avant d'√©crire un batch CSV (d√©faut: 100)
        keep_batches (bool): Garder les fichiers batch temporaires apr√®s fusion (d√©faut: False)
    
    Returns:
        dict: R√©sultats du traitement avec statistiques
    """
    fits_dir = Path(fits_dir)
    output_path = Path(output_file)
    
    # Cr√©er un dossier temporaire pour les fichiers batch
    batch_dir = output_path.parent / 'temp_batches'
    batch_dir.mkdir(parents=True, exist_ok=True)
    
    # Trouver tous les fichiers FITS
    fits_files = list(fits_dir.glob('*.fits'))
    
    print(f"Trouv√© {len(fits_files)} fichiers FITS √† traiter")
    print(f"Fichier de sortie final: {output_file}")
    print(f"Dossier temporaire: {batch_dir}")
    print(f"Batch size: {batch_size} fichiers (pour optimiser la m√©moire)")
    
    # Statistiques
    stats = {
        'total': len(fits_files),
        'success': 0,
        'failed': 0,
        'total_rows': 0
    }
    
    # Nettoyer les anciens fichiers batch s'ils existent
    for old_batch in batch_dir.glob('batch_*.csv'):
        old_batch.unlink()
    
    # S'assurer que le dossier parent du fichier final existe
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    all_dataframes = []
    batches_written = 0
    batch_files = []  # Liste des fichiers batch cr√©√©s
    failed_files = []  # Stocker uniquement les noms de fichiers √©chou√©s
    
    # Traitement en parall√®le avec √©criture par batch dans des fichiers s√©par√©s
    if progress_bar:
        with tqdm(total=len(fits_files), desc="Extraction FITS ‚Üí Batch CSV") as pbar:
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_file = {
                    executor.submit(extract_dataframe_from_fits, fits_file): fits_file 
                    for fits_file in fits_files
                }
                
                for future in concurrent.futures.as_completed(future_to_file):
                    df, result = future.result()
                    
                    if result['status'] == 'success' and df is not None:
                        all_dataframes.append(df)
                        stats['success'] += 1
                        stats['total_rows'] += len(df)
                        pbar.set_postfix_str(f"‚úì Batch: {len(all_dataframes)}/{batch_size} | Batches: {batches_written}")
                    else:
                        stats['failed'] += 1
                        failed_files.append(result['filename'])
                        pbar.set_postfix_str(f"‚úó Failed: {stats['failed']}")
                    
                    # √âcrire le batch dans un fichier s√©par√© quand il atteint la taille d√©finie
                    if len(all_dataframes) >= batch_size:
                        batches_written += 1
                        batch_file = batch_dir / f'batch_{batches_written:04d}.csv'
                        
                        batch_df = pd.concat(all_dataframes, ignore_index=True)
                        batch_df.to_csv(batch_file, index=False)
                        batch_files.append(batch_file)
                        
                        pbar.set_postfix_str(f"üíæ Batch {batches_written} √©crit ({len(batch_df)} rows) | Nettoyage...")
                        
                        # Lib√©ration agressive de la m√©moire
                        del all_dataframes
                        del batch_df
                        all_dataframes = []
                        gc.collect()  # Force garbage collection
                        
                        # Petit d√©lai pour laisser le syst√®me respirer
                        time.sleep(0.3)
                    
                    pbar.update(1)
    else:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(extract_dataframe_from_fits, fits_file) for fits_file in fits_files]
            for future in concurrent.futures.as_completed(futures):
                df, result = future.result()
                
                if result['status'] == 'success' and df is not None:
                    all_dataframes.append(df)
                    stats['success'] += 1
                    stats['total_rows'] += len(df)
                    
                    # √âcrire le batch dans un fichier s√©par√© quand il atteint la taille d√©finie
                    if len(all_dataframes) >= batch_size:
                        batches_written += 1
                        batch_file = batch_dir / f'batch_{batches_written:04d}.csv'
                        
                        batch_df = pd.concat(all_dataframes, ignore_index=True)
                        batch_df.to_csv(batch_file, index=False)
                        batch_files.append(batch_file)
                        
                        print(f"üíæ Batch {batches_written} √©crit ({len(batch_df)} rows) | Nettoyage m√©moire...")
                        
                        # Lib√©ration agressive de la m√©moire
                        del all_dataframes
                        del batch_df
                        all_dataframes = []
                        gc.collect()  # Force garbage collection
                        time.sleep(0.3)
                else:
                    stats['failed'] += 1
                    failed_files.append(result['filename'])
    
    # √âcrire le dernier batch restant (s'il y en a un)
    if all_dataframes:
        batches_written += 1
        batch_file = batch_dir / f'batch_{batches_written:04d}.csv'
        
        print(f"\nüíæ √âcriture du dernier batch ({len(all_dataframes)} DataFrames)...")
        batch_df = pd.concat(all_dataframes, ignore_index=True)
        batch_df.to_csv(batch_file, index=False)
        batch_files.append(batch_file)
        
        # Lib√©ration finale de la m√©moire
        del all_dataframes
        del batch_df
        gc.collect()
    
    # PHASE 2 : Fusion de tous les fichiers batch en un seul CSV
    print(f"\n{'='*70}")
    print(f"üì¶ PHASE 2 : FUSION DES BATCHES")
    print(f"{'='*70}")
    print(f"Nombre de fichiers batch √† fusionner : {len(batch_files)}")
    print(f"Fichier de sortie final : {output_file}")
    
    if batch_files:
        # Supprimer l'ancien fichier de sortie s'il existe
        if output_path.exists():
            output_path.unlink()
        
        # Fusionner tous les fichiers batch
        first_batch = True
        with tqdm(total=len(batch_files), desc="Fusion des batch CSV ‚Üí Fichier final") as pbar:
            for batch_file in batch_files:
                # Lire le batch par chunks pour √©conomiser la m√©moire
                for chunk in pd.read_csv(batch_file, chunksize=10000):
                    chunk.to_csv(
                        output_file,
                        mode='a' if not first_batch else 'w',
                        header=first_batch,
                        index=False
                    )
                    first_batch = False
                
                pbar.update(1)
                pbar.set_postfix_str(f"Fusionn√© {pbar.n}/{len(batch_files)} batches")
        
        # Nettoyer les fichiers batch temporaires (sauf si keep_batches=True)
        if not keep_batches:
            print(f"\nüßπ Nettoyage des fichiers temporaires...")
            for batch_file in batch_files:
                batch_file.unlink()
            
            # Supprimer le dossier temporaire s'il est vide
            try:
                batch_dir.rmdir()
                print(f"‚úÖ Dossier temporaire supprim√© : {batch_dir}")
            except:
                print(f"‚ö†Ô∏è  Le dossier temporaire contient encore des fichiers : {batch_dir}")
        else:
            print(f"\nüì¶ Fichiers batch conserv√©s dans : {batch_dir}")
            print(f"   ‚Ä¢ Nombre de fichiers : {len(batch_files)}")
        
        gc.collect()  # Dernier nettoyage m√©moire
    
    # V√©rifier et afficher les r√©sultats
    if output_path.exists():
        print(f"\n‚úÖ Fichier CSV cr√©√©: {output_file}")
        print(f"   ‚Ä¢ Taille: {output_path.stat().st_size / (1024**2):.2f} MB")
        print(f"   ‚Ä¢ Batches √©crits: {batches_written}")
        print(f"   ‚Ä¢ Lignes totales: {stats['total_rows']:,}")
        
        # Compter les colonnes en lisant juste l'en-t√™te
        sample_df = pd.read_csv(output_file, nrows=0)
        print(f"   ‚Ä¢ Colonnes: {len(sample_df.columns)}")
        del sample_df
    else:
        print("\n‚ùå Aucun fichier CSV g√©n√©r√©")
    
    # Afficher quelques fichiers √©chou√©s si n√©cessaire
    if failed_files:
        print(f"\n‚ö†Ô∏è  Fichiers √©chou√©s: {len(failed_files)}")
        if len(failed_files) <= 10:
            for f in failed_files:
                print(f"   ‚Ä¢ {f}")
        else:
            for f in failed_files[:5]:
                print(f"   ‚Ä¢ {f}")
            print(f"   ... et {len(failed_files) - 5} autres")
    
    return {
        'stats': stats,
        'failed_files': failed_files[:100] if len(failed_files) > 100 else failed_files,  # Limiter √† 100
        'output_file': output_file if output_path.exists() else None
    }


def get_csv_from_fits():
    # Configuration ULTRA-OPTIMIS√âE pour √©viter les crashes WSL
    FITS_DIR = Path("data/TESS/fits")
    OUTPUT_FILE = 'data/final/all_lightcurves.csv'
    # Configuration tr√®s conservatrice pour garantir la stabilit√©
    WORKERS = 3  # Nombre r√©duit pour stabilit√© maximale
    BATCH_SIZE = 100  # Batch r√©duit pour lib√©ration fr√©quente de la m√©moire
    
    if not FITS_DIR.exists():
        print("‚ùå Dossier FITS introuvable: data/TESS/fits/")
        return
    
    # Compter les fichiers FITS
    fits_files = list(FITS_DIR.glob("*.fits"))
    
    print("=" * 70)
    print("TEST : EXTRACTION EN UN SEUL CSV (STRAT√âGIE MULTI-FICHIERS)")
    print("=" * 70)
    print(f"\nüìÅ Dossier FITS: {FITS_DIR}")
    print(f"üìä Nombre de fichiers FITS: {len(fits_files)}")
    print(f"üìÑ Fichier de sortie: {OUTPUT_FILE}")
    print(f"\n‚öôÔ∏è  Strat√©gie d'optimisation:")
    print(f"   ‚Ä¢ Phase 1: Extraction FITS ‚Üí Fichiers batch CSV")
    print(f"   ‚Ä¢ Phase 2: Fusion des batches ‚Üí Fichier final unique")
    print(f"\nüîß Configuration:")
    print(f"   ‚Ä¢ Workers: {WORKERS} (stabilit√© maximale)")
    print(f"   ‚Ä¢ Batch size: {BATCH_SIZE} fichiers/batch")
    print(f"   ‚Ä¢ Garbage collection: Activ√© apr√®s chaque batch")
    print(f"   ‚Ä¢ D√©lai entre batches: 0.3s")
    print(f"   ‚Ä¢ Nombre de batches attendus: ~{len(fits_files) // BATCH_SIZE + 1}")
    
    if len(fits_files) == 0:
        print("\n‚ùå Aucun fichier FITS trouv√©!")
        return
    
    # Lancer le traitement
    print(f"\n‚è≥ Traitement en cours (optimis√© pour √©viter les crashes WSL)...\n")
    
    result = process_all_fits_single_csv(
        fits_dir=FITS_DIR,
        output_file=OUTPUT_FILE,
        max_workers=WORKERS,
        progress_bar=True,
        batch_size=BATCH_SIZE
    )
    
    # Afficher les r√©sultats
    stats = result['stats']
    
    print("\n" + "=" * 70)
    print("R√âSULTATS")
    print("=" * 70)
    print(f"\nüìä Statistiques:")
    print(f"   ‚Ä¢ Fichiers FITS trait√©s: {stats['success']} / {stats['total']}")
    print(f"   ‚Ä¢ √âchecs: {stats['failed']}")
    print(f"   ‚Ä¢ Lignes totales: {stats['total_rows']:,}")
    
    # V√©rifier le fichier CSV g√©n√©r√©
    if result.get('output_file') and Path(result['output_file']).exists():
        print(f"\n‚úÖ Fichier CSV cr√©√©: {result['output_file']}")
        
        # Charger et analyser le CSV
        df = pd.read_csv(result['output_file'])
        
        print(f"\nüîç Analyse du CSV:")
        print(f"   ‚Ä¢ Nombre de lignes: {len(df):,}")
        print(f"   ‚Ä¢ Nombre de colonnes: {len(df.columns)}")
        print(f"   ‚Ä¢ Colonnes: {list(df.columns)}")
        
        # V√©rifications
        print(f"\n‚úÖ V√©rifications:")
        print(f"   ‚Ä¢ Colonne 'time' pr√©sente: {'time' in df.columns or 'TIME' in df.columns}")
        print(f"   ‚Ä¢ Colonne 'TIC' pr√©sente: {'TIC' in df.columns}")
        print(f"   ‚Ä¢ Colonne 'SECTOR' pr√©sente: {'SECTOR' in df.columns}")
        
        if 'TIC' in df.columns:
            unique_tics = df['TIC'].nunique()
            print(f"   ‚Ä¢ Nombre de TIC uniques: {unique_tics}")
            print(f"   ‚Ä¢ TIC min: {df['TIC'].min()}")
            print(f"   ‚Ä¢ TIC max: {df['TIC'].max()}")
        
        if 'SECTOR' in df.columns:
            unique_sectors = df['SECTOR'].nunique()
            print(f"   ‚Ä¢ Nombre de SECTOR uniques: {unique_sectors}")
            print(f"   ‚Ä¢ Secteurs: {sorted(df['SECTOR'].unique())}")
        
        # Afficher un √©chantillon
        print(f"\nüìã √âchantillon de donn√©es (3 premi√®res lignes):")
        # Afficher quelques colonnes importantes
        cols_to_show = []
        for col in ['time', 'TIME', 'flux', 'FLUX', 'TIC', 'SECTOR']:
            if col in df.columns:
                cols_to_show.append(col)
        
        if cols_to_show:
            print(df[cols_to_show].head(3).to_string(index=False))
        else:
            print(df.head(3).to_string(index=False))
        
        # Taille du fichier
        file_size = Path(result['output_file']).stat().st_size
        print(f"\nüíæ Taille du fichier: {file_size / (1024**2):.2f} MB")
        
    else:
        print("\n‚ùå Aucun fichier CSV g√©n√©r√©")
    
    print("\n" + "=" * 70)

if __name__ == '__main__':
    get_csv_from_fits()
